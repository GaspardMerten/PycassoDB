{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Window Sliding Processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Idea: use a sliding window going through all tuples of the same train, divide into individual traject using the timestamps, and compare the previous tuples of the sliding window to decide on different flag to add to the tuple:\n",
    "- if the train is stopped at the given moment in time of the tuple.\n",
    "  - use the speed of the train, and if it is below a certain threshold, then it is stopped.\n",
    "  - use the distance between the previous tuple and the current tuple, and if it is below a certain threshold, then it is stopped.\n",
    "  - use external information, such as the positions stations in Belgium\n",
    "  - could want to create a model that defines waiting points in the system\n",
    "  - note: might be useless, since we are using intervals more than moments in time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pandas\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# index is the first row\n",
    "df = pandas.read_csv('../assets/ar41_for_ulb.csv', sep=\";\", index_col=0)\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We have about 3 million tuples\n",
    "print(\"Number of tuples:\", len(df))\n",
    "\n",
    "# We have about 100 different trains\n",
    "train_ids = df['mapped_veh_id'].unique()\n",
    "print(\"Number of unique trains:\", len(train_ids))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Recover all rows with N/A values\n",
    "df_NA = df[df.isna().any(axis=1)]\n",
    "\n",
    "# Remove all N/A values\n",
    "df_no_NA = df.dropna()\n",
    "print(\"Number of tuples after removing N/A values:\", len(df_no_NA))\n",
    "print(\"Number of tuples removed:\", len(df) - len(df_no_NA))\n",
    "print(\"Number of unique trains after removing N/A values:\", len(df_no_NA['mapped_veh_id'].unique()))\n",
    "\n",
    "df_NA.drop(['mapped_veh_id', 'lat', 'lon', 'timestamps_UTC'], axis=1).describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the N/A values are only present in the PC2 columns. And when there is NaN value, then all PC2 related columns of the given row are NaN.\n",
    "\n",
    "This means that we could either:\n",
    "- remove all the rows with NaN values, because we do not have the linking of the two parts of the train\n",
    "- or use those rows only for PC1 specific analysis\n",
    "\n",
    "But overall, most of those NaN values come with columns that have no information on PC2, so we can remove them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Describe all columns but lat, lon, and timestamps_UTC\n",
    "df.drop(['mapped_veh_id', 'lat', 'lon', 'timestamps_UTC'], axis=1).describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All the temperatures should be compared to typical seasonal temperature to see if there are any outliers.\n",
    "\n",
    "Note that sensors can be faulty, and that the temperature can be wrong. We could therefore not remove the outliers, but rather use them to create a flag that indicates that the temperature is wrong to train a predictive model, that uses the temperature of the other fluids and the seasonal temperatures to determine the faultiness."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[['lat', 'lon']].describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that all the positions in the data are in the following geobox (49.38359,3.574995) to (51.30288,5.573039), which approximately corresponds to Belgium. This is a good sign, since we are only interested in the data of Belgium, there are no completely off the map outliers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot the rectangle on a map\n",
    "import folium\n",
    "\n",
    "# Create a map, centered on Belgium, don't allow to zoom\n",
    "m = folium.Map(\n",
    "    location=[50.503887, 4.469936],\n",
    "    # Easily readable tiles\n",
    "    tiles='cartodbpositron',\n",
    "    zoom_start=8,\n",
    "    zoom_control=False,\n",
    "    scrollWheelZoom=False,\n",
    "    dragging=False\n",
    ")\n",
    "\n",
    "# Add points to the map\n",
    "# chose some random points of the data\n",
    "points = np.random.choice(len(df), 100000, replace=False)\n",
    "\n",
    "for idx, row in df.iloc[points].iterrows():\n",
    "    folium.CircleMarker([row['lat'], row['lon']], radius=1, color='red').add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m\n",
    "# m.show_in_browser()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The random point chosen in the dataset showcase the fact that the data is only in Belgium.\n",
    "\n",
    "The only outliers we could have are therefore the data points that make no sense in terms of positions compared to the previous data point of the same train at about the same time. Can only be found by using a time window that compares the positions of the train at different moments in time.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Study Time Intervals"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preliminary Study"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make the timestamp a datetime object instead of a string\n",
    "df['timestamps_UTC'] = pandas.to_datetime(df['timestamps_UTC'])\n",
    "\n",
    "# We have data from 2022-08-22 14:48:25 to 2023-09-13 21:51:57\n",
    "print(\"Data ranges from\", df['timestamps_UTC'].min(), \"to\", df['timestamps_UTC'].max())\n",
    "print(\"Data spans:\", df['timestamps_UTC'].max() - df['timestamps_UTC'].min())\n",
    "\n",
    "# Number of tuples per train (avg: 192.000)\n",
    "train_tuples = df.groupby('mapped_veh_id').size()\n",
    "print(\"Tuples per unique train:\\n\" + str(train_tuples.describe()))\n",
    "\n",
    "# Number of tuples per train per day (avg: 1.100)\n",
    "df_day = df.copy()\n",
    "df_day['timestamps_UTC'] = df_day['timestamps_UTC'].dt.date\n",
    "train_tuples_per_day = df_day.groupby(['mapped_veh_id', 'timestamps_UTC']).size()\n",
    "print(\"\\nTuples per unique train per day:\\n\" + str(train_tuples_per_day.describe()))\n",
    "\n",
    "# Describe the time between the tuples of a given train for a given day\n",
    "df_range = df[['mapped_veh_id', 'timestamps_UTC']].copy()\n",
    "# Sort the tuples by time\n",
    "df_range = df_range.sort_values(by=['mapped_veh_id', 'timestamps_UTC'])\n",
    "# Compute the time difference between each tuple of a given train\n",
    "df_range['time_difference'] = df_range.groupby(['mapped_veh_id'])['timestamps_UTC'].diff().dropna()\n",
    "df_range['time_difference'].describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Conclusions:\n",
    "1. Notice that some trains do not have that many data tuples, and that the average is 190.000 tuples per train.\n",
    "2. Notice also that some trains do not have data for every day, and that the average is 1.100 tuples per train per day.\n",
    "\n",
    "Seeing how certain train have close to zero information per day, let's assume that the train is not running on those days. A small amount of data is a risk for the model, since it might not be able to learn anything from it. We will therefore remove all the days when the train has less than a given amount of tuples.\n",
    "\n",
    "We should therefore define two thresholds:\n",
    "- The minimum amount of tuples per train per day\n",
    "- The minimum amount of tuples per train\n",
    "\n",
    "We shall then remove the tuples that do not respect these thresholds."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Estimate distance and speed between tuples\n",
    "\n",
    "Using the timestamps between each tuple of a given train, we can estimate the distance and speed between each tuple of a given train."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# vectorized haversine function\n",
    "def haversine(lat1, lon1, lat2, lon2, to_radians=True, earth_radius=6371):\n",
    "    \"\"\"\n",
    "    slightly modified version: of http://stackoverflow.com/a/29546836/2901002\n",
    "\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees or in radians)\n",
    "\n",
    "    All (lat, lon) coordinates must have numeric dtypes and be of equal length.\n",
    "    \"\"\"\n",
    "    if to_radians:\n",
    "        lat1, lon1, lat2, lon2 = np.radians([lat1, lon1, lat2, lon2])\n",
    "\n",
    "    a = np.sin((lat2 - lat1) / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin((lon2 - lon1) / 2.0) ** 2\n",
    "\n",
    "    return earth_radius * 2 * np.arcsin(np.sqrt(a))  # return the value in km since the earth_radius is in km"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: add the different columns that can be estimated from the data\n",
    "#   - speed in the interval\n",
    "#   - distance from last point\n",
    "\n",
    "# if the processed.csv file already exists, then load it\n",
    "try:\n",
    "    processed_df = pandas.read_csv('../assets/processed.csv', sep=\";\")\n",
    "    processed_df['timestamps_UTC'] = pandas.to_datetime(processed_df['timestamps_UTC'])\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found, creating it...\")\n",
    "\n",
    "    processed_df = df.copy()\n",
    "\n",
    "    processed_df = processed_df.dropna()\n",
    "\n",
    "    # Compute the time interval between each tuple of a given train\n",
    "    processed_df = processed_df.sort_values(by=['mapped_veh_id', 'timestamps_UTC'])\n",
    "    processed_df['time_difference'] = processed_df.groupby(['mapped_veh_id'])['timestamps_UTC'].diff().dropna()\n",
    "\n",
    "    # Replace N/A values with 0 seconds\n",
    "    processed_df['time_difference'] = processed_df['time_difference'].fillna(pandas.Timedelta(seconds=0))\n",
    "\n",
    "    # Compute the relative distance and average speed between each tuple for each train separately\n",
    "    for vehicle in processed_df['mapped_veh_id'].unique():\n",
    "        # Get the index of the tuples of the given train\n",
    "        vehicle_idx = processed_df[processed_df['mapped_veh_id'] == vehicle].index\n",
    "\n",
    "        # Compute the distance between each tuple of the given train\n",
    "        processed_df.loc[vehicle_idx, 'distance'] = haversine(\n",
    "            processed_df.loc[vehicle_idx, 'lat'].shift(),\n",
    "            processed_df.loc[vehicle_idx, 'lon'].shift(),\n",
    "            processed_df.loc[vehicle_idx, 'lat'],\n",
    "            processed_df.loc[vehicle_idx, 'lon']\n",
    "        ) * 1000  # multiplied by 1000 to have it in meters instead of kilometers\n",
    "        # Replace the first distance with 0\n",
    "        processed_df.loc[vehicle_idx[0], 'distance'] = 0\n",
    "\n",
    "        # Compute the speed between each tuple of the given train\n",
    "        processed_df.loc[vehicle_idx, 'speed'] = processed_df.loc[vehicle_idx, 'distance'] / processed_df.loc[\n",
    "            vehicle_idx, 'time_difference'].dt.total_seconds()  # in m/s, should\n",
    "        # Replace the first speed with 0\n",
    "        processed_df.loc[vehicle_idx[0], 'speed'] = 0\n",
    "\n",
    "    # Store the processed dataframe\n",
    "    processed_df.to_csv('../assets/processed.csv', sep=\";\", index=False)\n",
    "\n",
    "processed_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a csv file for each train in the in the dataset\n",
    "df = pd.read_csv('../assets/processed.csv', sep=';')\n",
    "\n",
    "# Group by train\n",
    "grouped = df.groupby('mapped_veh_id')\n",
    "\n",
    "# Iterate over each train\n",
    "for train_id, train_df in grouped:\n",
    "    # Save the train in a csv file\n",
    "    train_df.to_csv(f'../assets/trains/{int(train_id)}.csv', sep=';', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Study of the sensors\n",
    "\n",
    "Define analytical rules to determine if a sensor is faulty or not."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 1: Study the distribution of the sensors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Describe the temperature of the different fluids\n",
    "print(processed_df[['RS_E_InAirTemp_PC1', 'RS_E_InAirTemp_PC2']].describe())\n",
    "\n",
    "print(processed_df[['RS_T_OilTemp_PC1', 'RS_T_OilTemp_PC2']].describe())\n",
    "\n",
    "print(processed_df[['RS_E_WatTemp_PC1', 'RS_E_WatTemp_PC2']].describe())\n",
    "\n",
    "# Describe the pressure of the different fluids\n",
    "print(processed_df[['RS_E_OilPress_PC1', 'RS_E_OilPress_PC2']].describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For each vehicle id, plot the evolution RS_E_RPM_PC1\n",
    "# For each vehicle id, plot the evolution RS_E_RPM_PC2\n",
    "for x, (vehicle, group) in enumerate(processed_df.groupby(\"mapped_veh_id\")):\n",
    "    slice = group[50:100]\n",
    "    # slice.plot(x=\"timestamps_UTC\", y=[\"RS_E_OilPress_PC1\", \"RS_E_OilPress_PC2\"], ylim=[0, 500], title=\"Vehicle {}\".format(vehicle))\n",
    "\n",
    "# print(processed_df[processed_df['mapped_veh_id'] == 125][['RS_E_RPM_PC1', 'RS_E_RPM_PC2']].iloc[50:100])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ENGINE RPM\n",
    "# From the data, we see that the engine RPM is always between 0 and 2000.\n",
    "RPM = [0, 2000]\n",
    "# We also see that the nominal value is around 800.\n",
    "# Using this information, we can consider the running RPM range of the engines to be between 700 and 900 RPM.\n",
    "RPM_NOMINAL = [0, 2400]\n",
    "\n",
    "# outside_range =  processed_df[((processed_df['RS_E_RPM_PC1'] < RPM_NOMINAL[0]) | (processed_df['RS_E_RPM_PC1'] > RPM_NOMINAL[1])) & ((processed_df['RS_E_RPM_PC1'] != 0) | (processed_df['RS_E_RPM_PC1'] != 0))]\n",
    "# print(len(outside_range))\n",
    "#\n",
    "# outside_range = processed_df[((processed_df['RS_E_RPM_PC2'] < RPM_NOMINAL[0]) | (processed_df['RS_E_RPM_PC2'] > RPM_NOMINAL[1])) & ((processed_df['RS_E_RPM_PC2'] != 0) | (processed_df['RS_E_RPM_PC2'] != 0))]\n",
    "# print(len(outside_range))\n",
    "# print(outside_range[['RS_E_RPM_PC1', 'RS_E_RPM_PC2']])\n",
    "\n",
    "\n",
    "# PRESSURE\n",
    "# If the pressure is below 0, then the sensor is faulty\n",
    "# Note: pressure drops to zero only if engine is stopped, so if both sensors are 0, then it may have sense\n",
    "#  to assume that the engine is stopped, and therefore that the pressure is 0. But, if only one sensor is 0,\n",
    "#  then ont of them is probably faulty.\n",
    "OIL_PRESSURE = [0, 690]  # min and max value in the data\n",
    "\n",
    "# TEMPERATURE\n",
    "# All temperatures must be in a functional range,\n",
    "AIR = [0, 65]\n",
    "WATER = [0, 100]\n",
    "OIL = [0, 115]\n",
    "# The maximum acceptable temperatures are for the:\n",
    "#   - air: 65°C,\n",
    "#   - water: 100°C,\n",
    "#   - oil: 115°C.\n",
    "# Above those thresholds the engines are stopped automatically to avoid damage.\n",
    "\n",
    "# AIR TEMPERATURE\n",
    "# Air input from outside, so it is normal to have a low temperature in winter by example\n",
    "# Detection of low temperature can be used to detect a faulty sensor, typically, if both detect low\n",
    "# temperature, then it is probably normal, since they take it from outside the train engine,\n",
    "# but if only one detects low temperature, then one of them is probably faulty,\n",
    "# most certainly the one sending zero, but could also be a difference that make no sense and no zero value.\n",
    "\n",
    "# OIL TEMPERATURE and WATER TEMPERATURE\n",
    "# Both are in closed loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add Weather Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "city_locations = {\n",
    "    \"hasselt\": [50.930965, 5.338333],\n",
    "    \"brussels\": [50.85045, 4.34878],\n",
    "    \"antwerp\": [51.21989, 4.40346],\n",
    "    \"ghent\": [51.05, 3.71667],\n",
    "    \"charleroi\": [50.41136, 4.44448],\n",
    "}\n",
    "\n",
    "cities_df = pd.DataFrame(\n",
    "    data={\n",
    "        \"city\": [key for key in city_locations.keys()],\n",
    "        \"lat\": [city_locations[key][0] for key in city_locations.keys()],\n",
    "        \"lon\": [city_locations[key][1] for key in city_locations.keys()]\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a GeoDataFrame for your data tuples\n",
    "# Example: Assuming you have a DataFrame 'data_df' with columns 'latitude' and 'longitude'\n",
    "data_geometry = [Point(xy) for xy in zip(processed_df['lat'], processed_df['lon'])]\n",
    "data_gdf = gpd.GeoDataFrame(processed_df, geometry=data_geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# Create a GeoDataFrame for cities with their locations\n",
    "# Example: Assuming you have a DataFrame 'cities_df' with columns 'city_name', 'latitude', and 'longitude'\n",
    "cities_geometry = [Point(xy) for xy in zip(cities_df['lat'], cities_df['lon'])]\n",
    "cities_gdf = gpd.GeoDataFrame(cities_df, geometry=cities_geometry, crs=\"EPSG:4326\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform a spatial join to find the closest city for each data tuple\n",
    "# 'op' parameter defines the spatial relationship (in this case, it finds the nearest city)\n",
    "result_gdf = gpd.sjoin_nearest(data_gdf, cities_gdf, how=\"left\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display or use the result DataFrame as needed\n",
    "print(result_gdf.head())\n",
    "result_df = result_gdf[[processed_df.colnames, \"city\"]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T12:01:23.098933Z",
     "start_time": "2023-11-28T12:01:02.046380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "          mapped_veh_id      timestamps_UTC        lat       lon  \\\n0                 102.0 2023-01-23 07:25:08  51.017864  3.769079   \n1                 102.0 2023-01-23 07:25:16  51.017875  3.769046   \n2                 102.0 2023-01-23 07:25:37  51.017208  3.770179   \n3                 102.0 2023-01-23 07:25:41  51.016916  3.771036   \n4                 102.0 2023-01-23 07:26:10  51.016503  3.772182   \n...                 ...                 ...        ...       ...   \n17666542          197.0 2023-09-13 17:33:03  50.402693  4.450111   \n17666543          197.0 2023-09-13 17:33:58  50.401657  4.452693   \n17666544          197.0 2023-09-13 17:34:03  50.401830  4.452217   \n17666545          197.0 2023-09-13 17:34:58  50.401057  4.455388   \n17666546          197.0 2023-09-13 17:35:04  50.401084  4.455169   \n\n          RS_E_InAirTemp_PC1  RS_E_InAirTemp_PC2  RS_E_OilPress_PC1  \\\n0                       17.0                18.0              210.0   \n1                       17.0                20.0              200.0   \n2                       19.0                20.0              193.0   \n3                       19.0                20.0              196.0   \n4                       19.0                21.0              200.0   \n...                      ...                 ...                ...   \n17666542                37.0                40.0              220.0   \n17666543                37.0                38.0              224.0   \n17666544                37.0                38.0              224.0   \n17666545                36.0                38.0              207.0   \n17666546                36.0                38.0              213.0   \n\n          RS_E_OilPress_PC2  RS_E_RPM_PC1  RS_E_RPM_PC2  RS_E_WatTemp_PC1  \\\n0                     210.0         858.0         839.0              78.0   \n1                     200.0         801.0         804.0              79.0   \n2                     207.0         803.0         808.0              80.0   \n3                     203.0         801.0         803.0              80.0   \n4                     203.0         795.0         807.0              80.0   \n...                     ...           ...           ...               ...   \n17666542              258.0         803.0         803.0              81.0   \n17666543              307.0         843.0         941.0              80.0   \n17666544              307.0         841.0         932.0              80.0   \n17666545              244.0         800.0         803.0              81.0   \n17666546              244.0         802.0         801.0              82.0   \n\n          RS_E_WatTemp_PC2  RS_T_OilTemp_PC1  RS_T_OilTemp_PC2  \\\n0                     80.0              71.0              79.0   \n1                     80.0              76.0              79.0   \n2                     81.0              79.0              81.0   \n3                     81.0              79.0              81.0   \n4                     82.0              79.0              79.0   \n...                    ...               ...               ...   \n17666542              79.0              77.0              81.0   \n17666543              78.0              77.0              80.0   \n17666544              78.0              77.0              80.0   \n17666545              80.0              77.0              82.0   \n17666546              80.0              79.0              82.0   \n\n          time_difference    distance      speed       city  \n0         0 days 00:00:00    0.000000   0.000000      ghent  \n1         0 days 00:00:08    2.573768   0.321721      ghent  \n2         0 days 00:00:21  108.516678   5.167461      ghent  \n3         0 days 00:00:04   68.192469  17.048117      ghent  \n4         0 days 00:00:29   92.401557   3.186261      ghent  \n...                   ...         ...        ...        ...  \n17666542  0 days 00:00:05   11.756205   2.351241  charleroi  \n17666543  0 days 00:00:55  216.286719   3.932486  charleroi  \n17666544  0 days 00:00:05   38.787277   7.757455  charleroi  \n17666545  0 days 00:00:55  240.619860   4.374907  charleroi  \n17666546  0 days 00:00:06   15.864746   2.644124  charleroi  \n\n[17666547 rows x 18 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mapped_veh_id</th>\n      <th>timestamps_UTC</th>\n      <th>lat</th>\n      <th>lon</th>\n      <th>RS_E_InAirTemp_PC1</th>\n      <th>RS_E_InAirTemp_PC2</th>\n      <th>RS_E_OilPress_PC1</th>\n      <th>RS_E_OilPress_PC2</th>\n      <th>RS_E_RPM_PC1</th>\n      <th>RS_E_RPM_PC2</th>\n      <th>RS_E_WatTemp_PC1</th>\n      <th>RS_E_WatTemp_PC2</th>\n      <th>RS_T_OilTemp_PC1</th>\n      <th>RS_T_OilTemp_PC2</th>\n      <th>time_difference</th>\n      <th>distance</th>\n      <th>speed</th>\n      <th>city</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>102.0</td>\n      <td>2023-01-23 07:25:08</td>\n      <td>51.017864</td>\n      <td>3.769079</td>\n      <td>17.0</td>\n      <td>18.0</td>\n      <td>210.0</td>\n      <td>210.0</td>\n      <td>858.0</td>\n      <td>839.0</td>\n      <td>78.0</td>\n      <td>80.0</td>\n      <td>71.0</td>\n      <td>79.0</td>\n      <td>0 days 00:00:00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>ghent</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>102.0</td>\n      <td>2023-01-23 07:25:16</td>\n      <td>51.017875</td>\n      <td>3.769046</td>\n      <td>17.0</td>\n      <td>20.0</td>\n      <td>200.0</td>\n      <td>200.0</td>\n      <td>801.0</td>\n      <td>804.0</td>\n      <td>79.0</td>\n      <td>80.0</td>\n      <td>76.0</td>\n      <td>79.0</td>\n      <td>0 days 00:00:08</td>\n      <td>2.573768</td>\n      <td>0.321721</td>\n      <td>ghent</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102.0</td>\n      <td>2023-01-23 07:25:37</td>\n      <td>51.017208</td>\n      <td>3.770179</td>\n      <td>19.0</td>\n      <td>20.0</td>\n      <td>193.0</td>\n      <td>207.0</td>\n      <td>803.0</td>\n      <td>808.0</td>\n      <td>80.0</td>\n      <td>81.0</td>\n      <td>79.0</td>\n      <td>81.0</td>\n      <td>0 days 00:00:21</td>\n      <td>108.516678</td>\n      <td>5.167461</td>\n      <td>ghent</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>102.0</td>\n      <td>2023-01-23 07:25:41</td>\n      <td>51.016916</td>\n      <td>3.771036</td>\n      <td>19.0</td>\n      <td>20.0</td>\n      <td>196.0</td>\n      <td>203.0</td>\n      <td>801.0</td>\n      <td>803.0</td>\n      <td>80.0</td>\n      <td>81.0</td>\n      <td>79.0</td>\n      <td>81.0</td>\n      <td>0 days 00:00:04</td>\n      <td>68.192469</td>\n      <td>17.048117</td>\n      <td>ghent</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>102.0</td>\n      <td>2023-01-23 07:26:10</td>\n      <td>51.016503</td>\n      <td>3.772182</td>\n      <td>19.0</td>\n      <td>21.0</td>\n      <td>200.0</td>\n      <td>203.0</td>\n      <td>795.0</td>\n      <td>807.0</td>\n      <td>80.0</td>\n      <td>82.0</td>\n      <td>79.0</td>\n      <td>79.0</td>\n      <td>0 days 00:00:29</td>\n      <td>92.401557</td>\n      <td>3.186261</td>\n      <td>ghent</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17666542</th>\n      <td>197.0</td>\n      <td>2023-09-13 17:33:03</td>\n      <td>50.402693</td>\n      <td>4.450111</td>\n      <td>37.0</td>\n      <td>40.0</td>\n      <td>220.0</td>\n      <td>258.0</td>\n      <td>803.0</td>\n      <td>803.0</td>\n      <td>81.0</td>\n      <td>79.0</td>\n      <td>77.0</td>\n      <td>81.0</td>\n      <td>0 days 00:00:05</td>\n      <td>11.756205</td>\n      <td>2.351241</td>\n      <td>charleroi</td>\n    </tr>\n    <tr>\n      <th>17666543</th>\n      <td>197.0</td>\n      <td>2023-09-13 17:33:58</td>\n      <td>50.401657</td>\n      <td>4.452693</td>\n      <td>37.0</td>\n      <td>38.0</td>\n      <td>224.0</td>\n      <td>307.0</td>\n      <td>843.0</td>\n      <td>941.0</td>\n      <td>80.0</td>\n      <td>78.0</td>\n      <td>77.0</td>\n      <td>80.0</td>\n      <td>0 days 00:00:55</td>\n      <td>216.286719</td>\n      <td>3.932486</td>\n      <td>charleroi</td>\n    </tr>\n    <tr>\n      <th>17666544</th>\n      <td>197.0</td>\n      <td>2023-09-13 17:34:03</td>\n      <td>50.401830</td>\n      <td>4.452217</td>\n      <td>37.0</td>\n      <td>38.0</td>\n      <td>224.0</td>\n      <td>307.0</td>\n      <td>841.0</td>\n      <td>932.0</td>\n      <td>80.0</td>\n      <td>78.0</td>\n      <td>77.0</td>\n      <td>80.0</td>\n      <td>0 days 00:00:05</td>\n      <td>38.787277</td>\n      <td>7.757455</td>\n      <td>charleroi</td>\n    </tr>\n    <tr>\n      <th>17666545</th>\n      <td>197.0</td>\n      <td>2023-09-13 17:34:58</td>\n      <td>50.401057</td>\n      <td>4.455388</td>\n      <td>36.0</td>\n      <td>38.0</td>\n      <td>207.0</td>\n      <td>244.0</td>\n      <td>800.0</td>\n      <td>803.0</td>\n      <td>81.0</td>\n      <td>80.0</td>\n      <td>77.0</td>\n      <td>82.0</td>\n      <td>0 days 00:00:55</td>\n      <td>240.619860</td>\n      <td>4.374907</td>\n      <td>charleroi</td>\n    </tr>\n    <tr>\n      <th>17666546</th>\n      <td>197.0</td>\n      <td>2023-09-13 17:35:04</td>\n      <td>50.401084</td>\n      <td>4.455169</td>\n      <td>36.0</td>\n      <td>38.0</td>\n      <td>213.0</td>\n      <td>244.0</td>\n      <td>802.0</td>\n      <td>801.0</td>\n      <td>82.0</td>\n      <td>80.0</td>\n      <td>79.0</td>\n      <td>82.0</td>\n      <td>0 days 00:00:06</td>\n      <td>15.864746</td>\n      <td>2.644124</td>\n      <td>charleroi</td>\n    </tr>\n  </tbody>\n</table>\n<p>17666547 rows × 18 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
